{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Trees From Scratch\n",
    "I will being by looking at the example of a GB Tree for regression. I will use the data from the autotrader task that has already been split, cleaned and encoded with the aim of predicting the price of a car.  I will begin by using sklearn's decision tree algorithm as a base, rather than implementing my own tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Tree-based Algorithms\n",
    "\n",
    "Gradient Boosted Trees is another ensemble method, similar to a Random Forest algorithm. This means several tree-like models are trained in parallel and are all used to contribute to a final prediction. \n",
    "\n",
    "### Random Forests\n",
    "\n",
    "In a Random Forest (RF), the first step is to produce a bootstrapped dataset (sampling with replacement). We then build a decision tree using this bootstrapped dataset - crucially, we use a random subset of the features at each step rather than the entire feature space. These 2 steps are repeated to produce a 'forest' of these random trees. \n",
    "\n",
    "**Random Forests on New Data**\n",
    "\n",
    "When a new piece of data is passed through a RF, each tree provides an answer. In classification this could be yes/no, in regression this could be a continuous variable, like price. Once all the trees have given an answer, the final prediction is either the mode (in classification) or the mean (in regression) - this technique is called *bagging*. An out-of-bag score can then be calculated - the out-of-bag dataset are the examples that were not selected in the bootstrapping stage and acts as a validation set like in Cross Validation. In classification, the proportion of out-of-bag samples that were *incorrectly* classified is known as the out-of-bag error \n",
    "\n",
    "### Regression Trees\n",
    "The inital root for a regression tree is the threshold for a feature which minimises the sum of the squared residuals. Each leaf continues in this fashion until the tree can no longer be split. To prevent overfitting, the algorithm typically has a min_samples_split hyperparameter. This prevents a split with a single observation.\n",
    "\n",
    "# <a href=\"https://jerryfriedman.su.domains/ftp/trebst.pdf\">Gradient Boosted Trees</a>\n",
    "## Regression\n",
    "Gradient Boost (GB) starts with a single leaf for the inital guess - for a continuous variable, this will be the average of the target. From this inital guess, GB trees are constructed but their overall size (number of leaves) is limited - typically this is between 8 and 32. Each successive tree is built upon the previous trees errors and once the maximum number of trees are constructed, a linear combination of these trees are utilised to make predictions. Each successive tree is scaled by the same amount - a constant usually defined as the learning rate or $\\eta$.\n",
    "\n",
    " **AdaBoost** is a similar boosted tree algorithm - in this, however, the trees that are constructed are actually 'stumps' (this means a root and a single left and right leaf). Not only that but each successive stump is individually scaled based on how well it performed, rather than scalling all trees by a fixed $\\eta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>fav_colour</th>\n",
       "      <th>gender</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   height  fav_colour  gender  weight\n",
       "0     1.6           0       0      88\n",
       "1     1.6           1       1      76\n",
       "2     1.5           0       1      56\n",
       "3     1.8           2       0      73\n",
       "4     1.5           1       0      77\n",
       "5     1.4           0       1      57"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating some Dummy Data - from StatsQuest Video\n",
    "\n",
    "# encoding: blue = 0, green = 1, red = 2\n",
    "# encoding: male = 0 female = 1\n",
    "df = pd.DataFrame({\n",
    "    'height': [1.6, 1.6, 1.5, 1.8, 1.5, 1.4], \n",
    "    'fav_colour': [0, 1, 0, 2, 1, 0],\n",
    "    'gender': [0, 1, 1, 0, 0, 1],\n",
    "    'weight': [88, 76, 56, 73, 77, 57]\n",
    "    })\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['weight'].reset_index()\n",
    "X = df[['height', 'fav_colour', 'gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create an initial guess\n",
    "The initial guess or prediction in a regression GB tree is the mean of the target. For this simple example, the target is the weight of a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our inital guess is: 71.16666666666667kg -> (71.17kg to 2 d.p)\n"
     ]
    }
   ],
   "source": [
    "initial_guess = y['weight'].mean()\n",
    "print(f\"Our inital guess is: {initial_guess}kg -> ({initial_guess:.2f}kg to 2 d.p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate a pseudo-residual\n",
    "We can use our first prediction to calcuate our initial pseudo-residual. This can be calculated as (observed - predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>weight</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>16.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>4.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>-15.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>1.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>-14.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  weight   residual\n",
       "0      0      88  16.833333\n",
       "1      1      76   4.833333\n",
       "2      2      56 -15.166667\n",
       "3      3      73   1.833333\n",
       "4      4      77   5.833333\n",
       "5      5      57 -14.166667"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['residual'] = y['weight'] - initial_guess\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a tree to predict the residuals\n",
    "Each successive tree is built on the errors (or residuals) from the previous tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = DecisionTreeRegressor(random_state=RANDOM_SEED).fit(X, y['residual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create new predictions based on the newly built tree(s)\n",
    "Here I only have a single tree but typically, a large number of individual trees will be built and trained on successive residuals - the theory is that each successive tree adds small improvements to the overall prediction. The contribution of each new tree is controlled by a hyperparameter called the learning rate ($\\eta$) - this is a constant scaling factor to prevent extreme overfitting.\n",
    "\n",
    "AdaBoost, another popular gradient-boosted tree algorithm, has a dynamic 'learning rate' - this means each tree is scaled differently depending on how well the tree performed. \n",
    "\n",
    "\n",
    "Residuals are updated as follow:\n",
    "$\\textrm{y} - (\\textrm{initial prediction} + (\\textrm{learning rate} * \\textrm{tree's prediction}))$\n",
    "\n",
    "We can see below that with the creation of a new tree, our residuals have decreased by a small amount which means thats we have move a small step in the right direction. We can quantify this by calculating the sum of the residuals squared - we aim to minimise this as we build new trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sum of residuals squared : 8.077935669463161e-28\n",
      "New sum of residuals squared : 5.048709793414476e-29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>weight</th>\n",
       "      <th>residual</th>\n",
       "      <th>new_residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>16.833333</td>\n",
       "      <td>3.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>-15.166667</td>\n",
       "      <td>-3.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>-14.166667</td>\n",
       "      <td>-2.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  weight   residual  new_residuals\n",
       "0      0      88  16.833333       3.366667\n",
       "1      1      76   4.833333       0.966667\n",
       "2      2      56 -15.166667      -3.033333\n",
       "3      3      73   1.833333       0.366667\n",
       "4      4      77   5.833333       1.166667\n",
       "5      5      57 -14.166667      -2.833333"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typically this is a small fraction (e.g. 0.1 or 0.3) but I have a single tree so I have increased it\n",
    "learning_rate = 0.8\n",
    "new_person = pd.DataFrame(\n",
    "    {\n",
    "    'height': [1.7], \n",
    "    'fav_colour': [1],\n",
    "    'gender': [0],\n",
    "    'weight': [75]\n",
    "    }\n",
    ")\n",
    "\n",
    "y['new_residuals'] = y.weight - (initial_guess + (dt1.predict(X) * learning_rate))\n",
    "\n",
    "print(f\"Original sum of residuals squared : {sum(y.residual) ** 2}\")\n",
    "print(f\"New sum of residuals squared : {sum(y.new_residuals) ** 2}\")\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Full Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 156170\n",
      "Testing data size: 66930\n"
     ]
    }
   ],
   "source": [
    "# Load in data from autotrader data cleaner\n",
    "X_train = pd.read_csv(\"data/X_train.csv\").to_numpy()\n",
    "X_test = pd.read_csv(\"data/X_test.csv\").to_numpy()\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")\n",
    "\n",
    "print(f\"Training data size: {len(X_train)}\")\n",
    "print(f\"Testing data size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_TREES = 20\n",
    "ETA = 0.1\n",
    "\n",
    "def objective(residuals):\n",
    "    \"\"\"Sum of Squared Residuals\"\"\"\n",
    "    residual_sum = sum(residuals)\n",
    "    return residual_sum ** 2\n",
    "\n",
    "\n",
    "def train_gbt(X, y, num_of_trees, eta=0.1, target='price'):\n",
    "    # TODO re-write without need for target\n",
    "    y_copy = y.copy()\n",
    "\n",
    "    # Step 1: Produce initial guess - regression GB tree initial is mean of target\n",
    "    initial_guess = y_copy[target].mean()\n",
    "\n",
    "    if 'residual' not in y_copy.columns:\n",
    "        # Step 2: Produce initial pseudo-residual - this is calculated as observed - predicted\n",
    "        y_copy['residual'] = y_copy[target] - initial_guess\n",
    "\n",
    "        # Step 2.5: Calculate the sum of the residuals squared as a metric of training performance. We aim to minimise this\n",
    "        ssr = objective(y_copy['residual'])\n",
    "        print(f\"Inital sum of squared residuals: {ssr}\")\n",
    "\n",
    "    # Initialise a list to keep track of trained trees    \n",
    "    trees = []\n",
    "    for _ in range(num_of_trees):\n",
    "        # Limit max number of leaf nodes to 4 - this is predominantly for performance rather than overfitting\n",
    "        dt = DecisionTreeRegressor(random_state=RANDOM_SEED, max_leaf_nodes=4) \n",
    "\n",
    "        # Step 3: Build a tree and fit it to the residuals (i.e. the error of the previous tree)\n",
    "        dt.fit(X, y_copy['residual'])\n",
    "        trees.append(dt) \n",
    "\n",
    "        # Initialise 'adjustment' - this is the factor by which successive trees influence the initial guess\n",
    "        # This adjustment increases or decreases to get better predictions for each example\n",
    "        tree_adjustment = 0\n",
    "        for tree in trees:\n",
    "            # Each tree's contribution to a prediction is scaled by a constant scalar called the learning rate\n",
    "            # Each tree is scaled by same constant unlike AdaBoost which scales individual trees based on their performance\n",
    "            tree_contribution = tree.predict(X) * learning_rate\n",
    "            tree_adjustment += tree_contribution\n",
    "\n",
    "        y_copy['residual'] = np.array(y_copy[target] - (initial_guess + tree_adjustment))\n",
    "        \n",
    "\n",
    "    print(f'Final SSR: {objective(y_copy.residual)}')\n",
    "    # I return y_copy to see final residuals and a list of trained trees to feed to a predict function\n",
    "    return y_copy, trees\n",
    "\n",
    "\n",
    "def train_gbt_np(X, y):\n",
    "    # TODO re-write without need for target\n",
    "    residuals = y.copy().astype(float)\n",
    "\n",
    "    # Step 1: Produce initial guess - regression GB tree initial is mean of target\n",
    "    initial_guess = np.mean(residuals)\n",
    "\n",
    "    # Step 2: Produce initial pseudo-residual - this is calculated as observed - predicted\n",
    "    residuals -= initial_guess\n",
    "    # Step 2.5: Calculate the sum of the residuals squared as a metric of training performance. We aim to minimise this\n",
    "    ssr = objective(residuals)\n",
    "    print(f\"Inital sum of squared residuals: {ssr}\")\n",
    "\n",
    "    # Initialise a list to keep track of trained trees    \n",
    "    trees = []\n",
    "    for _ in range(NUM_OF_TREES):\n",
    "        # Limit max number of leaf nodes to 4 - this is predominantly for performance rather than overfitting\n",
    "        dt = DecisionTreeRegressor(random_state=RANDOM_SEED, max_leaf_nodes=4) \n",
    "\n",
    "        # Step 3: Build a tree and fit it to the residuals (i.e. the error of the previous tree)\n",
    "        dt.fit(X, residuals)\n",
    "        trees.append(dt)\n",
    "\n",
    "        # Initialise 'adjustment' - this is the factor by which successive trees influence the initial guess\n",
    "        # This adjustment increases or decreases to get better predictions for each example\n",
    "        tree_adjustment = 0\n",
    "        for tree in trees:\n",
    "            # Each tree's contribution to a prediction is scaled by a constant scalar called the learning rate\n",
    "            # Each tree is scaled by same constant unlike AdaBoost which scales individual trees based on their performance\n",
    "            tree_contribution = tree.predict(X) * ETA\n",
    "            tree_adjustment += tree_contribution\n",
    "\n",
    "        residuals -= (initial_guess + tree_adjustment)\n",
    "        \n",
    "    print(f'Final SSR: {objective(residuals)}')\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital sum of squared residuals: 7.0909709531298234e-12\n",
      "Final SSR: 3.143418974384534e-16\n"
     ]
    }
   ],
   "source": [
    "new_y, tree_list = train_gbt(X_train, y_train, NUM_OF_TREES, ETA)\n",
    "# tree_list = train_gbt_np(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbt_predict(X_test, trees):\n",
    "    # Reshape so we can broadcast\n",
    "    y_preds = np.full((len(X_test), 1), fill_value=y_train.mean()).reshape(-1, )\n",
    "    for tree in trees: \n",
    "        y_preds += learning_rate * tree.predict(X_test)\n",
    "    return y_preds  \n",
    "        \n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\" Assuming both as np arrays \"\"\"\n",
    "    diff_squared = (y_true - y_pred) ** 2\n",
    "    return diff_squared.mean()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return mse(y_true, y_pred) ** 0.5\n",
    "\n",
    "def gbt_score(y_true, y_pred):\n",
    "    y_true = y_true.to_numpy().reshape(-1, )\n",
    "    print(f\"RMSE: {rmse(y_true, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6825.83\n"
     ]
    }
   ],
   "source": [
    "y_preds = gbt_predict(X_test, tree_list)\n",
    "gbt_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "I have downloaded a simple **binary** classification dataset from Kaggle in which we try to predict whether someone has heart disease or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, the initial prediction for each example is the **log of the odds**. We can convert a log of the odds to a probability using the logit function seen below:\n",
    "\n",
    "$\\huge \\frac{e^x}{1 + e^x}$\n",
    "\n",
    "Where $x$ is the log of the odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_X = heart_df.loc[:, heart_df.columns != 'output']\n",
    "heart_y = heart_df.output\n",
    "\n",
    "heart_X_train, heart_X_test, heart_y_train, heart_y_test = train_test_split(heart_X, heart_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ETA = 0.1\n",
    "def get_log_odds(x, y):\n",
    "    return np.log(x / y)\n",
    "\n",
    "def logit(x):\n",
    "    # Used to convert log of odds -> probability\n",
    "    return np.exp(x) / (1 + np.exp(x))    \n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    y_pred_label = [round(pred) for pred in y_pred]\n",
    "    acc_score = accuracy_score(y_true, y_pred_label)\n",
    "    f1 = f1_score(y_true, y_pred_label)\n",
    "\n",
    "    return acc_score, f1\n",
    "\n",
    "def train_gbt_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "    This only trains a single tree to imporve predictions because the dataset is relatively small\n",
    "    \"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    util_df = pd.DataFrame()  # Will be used for storing residuals / probabilities\n",
    "\n",
    "    initial_log_odds = get_log_odds(class_counts[1], class_counts[0])\n",
    "    initial_probability = logit(initial_log_odds)\n",
    "    initial_predictions = [round(initial_probability) for _ in range(len(y_train))]\n",
    "\n",
    "    init_acc, init_f1 = score(y_train, initial_predictions)\n",
    "    print(f\"Initial Predictions\\nAcc -> {init_acc:.2f}\\nF1 -> {init_f1:.2f}\")\n",
    "\n",
    "    util_df['predicted_probability'] = [initial_probability for _ in range(len(y_train))]\n",
    "    util_df['residual'] = np.array(y_train) - util_df['predicted_probability']\n",
    "\n",
    "    dt = DecisionTreeRegressor(random_state=RANDOM_SEED, max_leaf_nodes=4).fit(X_train, util_df['residual'])\n",
    "\n",
    "    util_df['leaf_index'] = dt.apply(X_train)\n",
    "    util_df['one_minus_previous_probability'] = 1 - util_df['predicted_probability']\n",
    "\n",
    "    leaf_map = dict()\n",
    "    for ind in set(util_df['leaf_index']):\n",
    "        sub_df = util_df.loc[util_df['leaf_index'] == ind].reset_index()\n",
    "        residual_sum = sub_df['residual'].sum()\n",
    "        sub_df['denominator'] = sub_df['predicted_probability'] * sub_df['one_minus_previous_probability']\n",
    "        p_sum = sub_df['denominator'].sum()\n",
    "        leaf_map[ind] = residual_sum / p_sum\n",
    "\n",
    "    util_df['leaf_output'] = util_df.leaf_index.map(leaf_map) * ETA\n",
    "\n",
    "\n",
    "    new_log_odd_preds = initial_log_odds + util_df['leaf_output']\n",
    "    util_df['predicted_probability'] = logit(new_log_odd_preds)\n",
    "    util_df['residual'] = y_train - util_df['predicted_probability']\n",
    "\n",
    "    acc, f1 = score(y_train, util_df.predicted_probability)\n",
    "    print(f\"Final Predictions\\nAcc -> {acc:.2f}\\nF1 -> {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Predictions\n",
      "Acc -> 0.54\n",
      "F1 -> 0.70\n",
      "Final Predictions\n",
      "Acc -> 0.75\n",
      "F1 -> 0.81\n"
     ]
    }
   ],
   "source": [
    "train_gbt_classifier(heart_X_train, heart_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b419173de24494810e65c4ea4d4c859b912f4352999b480e23ee82152eb9c1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pyZak')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
